var documenterSearchIndex = {"docs":
[{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api/#Training-Core","page":"API Reference","title":"Training Core","text":"","category":"section"},{"location":"api/#Quantities","page":"API Reference","title":"Quantities","text":"","category":"section"},{"location":"api/#Dashboard-and-Visualization","page":"API Reference","title":"Dashboard and Visualization","text":"","category":"section"},{"location":"api/#LMD4MLTraining.Learner","page":"API Reference","title":"LMD4MLTraining.Learner","text":"Learner{M, D, F, P, Q}\n\nObject bundling together all information for training.\n\nmodel: Architecture optimized during training.\ndata_loader: Iterable for training data.\nloss_fn: Function calculating the loss.\noptim: Optimizer state.\nquantities: Metrics computed every training step.\n\n\n\n\n\n","category":"type"},{"location":"api/#LMD4MLTraining.train!","page":"API Reference","title":"LMD4MLTraining.train!","text":"train!(learner, epochs, with_plots)\n\nTrain a Learner for a number of epochs, optionally with live plotting.\n\n\n\n\n\n","category":"function"},{"location":"api/#LMD4MLTraining.train_loop!","page":"API Reference","title":"LMD4MLTraining.train_loop!","text":"train_loop!(learner, epochs, channel)\n\nInternal training loop that computes quantities and sends them to the display channel.\n\n\n\n\n\n","category":"function"},{"location":"api/#LMD4MLTraining.compute","page":"API Reference","title":"LMD4MLTraining.compute","text":"compute(q::AbstractQuantity, losses, back, grads, params)\n\nCompute the value of quantity q using the provided training information.\n\n\n\n\n\n","category":"function"},{"location":"api/#LMD4MLTraining.quantity_key","page":"API Reference","title":"LMD4MLTraining.quantity_key","text":"quantity_key(q::AbstractQuantity)\n\nReturn a symbol key uniquely identifying the quantity.\n\n\n\n\n\n","category":"function"},{"location":"api/#LMD4MLTraining.AbstractQuantity","page":"API Reference","title":"LMD4MLTraining.AbstractQuantity","text":"AbstractQuantity\n\nAbstract base type for all quantities tracked during training.\n\n\n\n\n\n","category":"type"},{"location":"api/#LMD4MLTraining.LossQuantity","page":"API Reference","title":"LMD4MLTraining.LossQuantity","text":"LossQuantity\n\nQuantity tracking the training loss.\n\n\n\n\n\n","category":"type"},{"location":"api/#LMD4MLTraining.DistanceQuantity","page":"API Reference","title":"LMD4MLTraining.DistanceQuantity","text":"DistanceQuantity\n\nQuantity tracking the L2 distance of current parameters from the initial parameters.\n\n\n\n\n\n","category":"type"},{"location":"api/#LMD4MLTraining.GradNormQuantity","page":"API Reference","title":"LMD4MLTraining.GradNormQuantity","text":"GradNormQuantity\n\nQuantity tracking the norm of the model gradients.\n\n\n\n\n\n","category":"type"},{"location":"api/#LMD4MLTraining.UpdateSizeQuantity","page":"API Reference","title":"LMD4MLTraining.UpdateSizeQuantity","text":"UpdateSizeQuantity\n\nQuantity tracking the L2 distance of parameters before and after the current update step.\n\n\n\n\n\n","category":"type"},{"location":"api/#LMD4MLTraining.NormTestQuantity","page":"API Reference","title":"LMD4MLTraining.NormTestQuantity","text":"NormTestQuantity\n\nQuantity tracking the normalized gradient noise.\n\n\n\n\n\n","category":"type"},{"location":"api/#LMD4MLTraining.GradHist1dQuantity","page":"API Reference","title":"LMD4MLTraining.GradHist1dQuantity","text":"GradHist1dQuantity\n\nHistogram of per-sample gradient elements.\n\n\n\n\n\n","category":"type"},{"location":"api/#LMD4MLTraining.build_dashboard","page":"API Reference","title":"LMD4MLTraining.build_dashboard","text":"build_dashboard(quantities) -> (fig, axes_dict)\n\nConstruct the dashboard layout for the given quantities and return the figure and axis mapping. Loss quantity is always plotted\n\n\n\n\n\n","category":"function"},{"location":"getting_started/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"This page shows how to install the package and run LMD4MLTraining.jl on a small MNIST example to visualize training dynamics in real time.\n\n","category":"section"},{"location":"getting_started/#Requirements","page":"Getting Started","title":"Requirements","text":"Julia\nA working Makie backend\n\n","category":"section"},{"location":"getting_started/#Get-the-code","page":"Getting Started","title":"Get the code","text":"Start a Julia REPL and add the package to the desired environment via: \n\njulia> using Pkg\njulia> Pkg.add(url=\"https://github.com/LJS42/LMD4MLTraining.jl\")\n\nNow you can load and use the package: \n\njulia> using LMD4MLTraining\n\nIf instead you want to clone the repository to a desired directory:\n\ngit clone <REPOSITORY_URL>\ncd <REPOSITORY_NAME>\n\nThen activate the project and install dependencies: \n\npkg> activate .\npkg> instantiate\n\n","category":"section"},{"location":"getting_started/#Quick-Start","page":"Getting Started","title":"Quick Start","text":"You can run the provided MNIST example to see the dashboard in action without cloning the repository. Copy and paste the following command into your terminal:\n\njulia -e 'using Pkg; Pkg.activate(temp=true); Pkg.add([Pkg.PackageSpec(url=\"https://github.com/LJS42/LMD4MLTraining.jl\"), Pkg.PackageSpec(name=\"Flux\"), Pkg.PackageSpec(name=\"MLDatasets\")]); include(download(\"https://raw.githubusercontent.com/LJS42/LMD4MLTraining.jl/main/examples/mnist.jl\"))'\n\nIf you have already cloned the repository, you can run it using:\n\njulia --project=. -e 'import Pkg; Pkg.instantiate(); include(\"examples/mnist.jl\")'\n\nAlternatively, include it in your own training loop:\n\nusing LMD4MLTraining\nusing Flux\n\n# Define your model, data, loss, and optimizer\n# ...\n\n# Setup learner with quantities\nquantities = [LossQuantity(), GradNormQuantity(), DistanceQuantity()]\nlearner = Learner(model, data_loader, loss_fn, optim, quantities)\n\n# Train with live plotting\ntrain!(learner; epochs=10, with_plots=true, track_every=1)","category":"section"},{"location":"getting_started/#MNIST-Live-Monitoring-Example","page":"Getting Started","title":"MNIST Live Monitoring Example","text":"To get started, you can run the provided MNIST example. This example demonstrates how to integrate LMD4MLTraining.jl into a Flux.jl training loop.\n\nusing LMD4MLTraining\nusing Flux\nusing MLDatasets\n\n# 1. Prepare data loader\nfunction get_data_loader()\n    preprocess(x, y) = (reshape(x, 28, 28, 1, :), Flux.onehotbatch(y, 0:9))\n    x_train_raw, y_train_raw = MLDatasets.MNIST.traindata()\n    x_train, y_train = preprocess(x_train_raw, y_train_raw)\n    return Flux.DataLoader((x_train, y_train); batchsize=16, shuffle=true)\nend\n\n# 2. Define model\nmodel = Chain(\n    Conv((5, 5), 1 => 6, relu),\n    MaxPool((2, 2)),\n    Conv((5, 5), 6 => 16, relu),\n    MaxPool((2, 2)),\n    Flux.flatten,\n    Dense(256, 120, relu),\n    Dense(120, 84, relu),\n    Dense(84, 10),\n)\n\n# 3. Setup optimizer and loss\noptim = Flux.setup(Adam(3f-4), model)\nloss_fn(ŷ, y) = vec(Flux.logitcrossentropy(ŷ, y; agg=identity))\n\n# 4. Define quantities to track\nquantities = LMD4MLTraining.AbstractQuantity[\n    LossQuantity(), \n    GradNormQuantity(), \n    DistanceQuantity(), \n    UpdateSizeQuantity(), \n    GradNormQuantity()]\n#quantities = LMD4MLTraining.AbstractQuantity[] -> if no quantities selected, loss quantity will be plotted\n\n# 5. Create Learner and start training with plots\nlearner = Learner(model, get_data_loader(), loss_fn, optim, quantities)\ntrain!(learner, 5, true, 20) # Train for 5 epochs with live plotting every 20th step\n\nWhen you run this code, a browser window will automatically open with a live dashboard showing the training progress.","category":"section"},{"location":"architecture/#Architecture","page":"Architecture","title":"Architecture","text":"This page describes the internal architecture of LMD4MLTraining.jl.\n\n","category":"section"},{"location":"architecture/#Overview","page":"Architecture","title":"Overview","text":"The package is organized into the following components:\n\nTraining backend (Flux integration)\nQuantities (metrics computed during training)\nVisualization (Makie-based dashboard)\nSession management\n\n","category":"section"},{"location":"architecture/#Module-structure","page":"Architecture","title":"Module structure","text":"quantities/: defines the AbstractQuantity interface and specific metrics (loss, gradients, etc.)\ninstruments/: contains the renderer and dashboard for live visualization.\nlearner.jl: provides the Learner abstraction and the training loop integrations.\n\nThis modular design allows new quantities and visual instruments to be added without modifying the core training logic.","category":"section"},{"location":"architecture/#Core-Concepts","page":"Architecture","title":"Core Concepts","text":"The central object in LMD4MLTraining is the Learner. It bundles everything needed for training and monitoring:\n\nModel: The Flux model to be trained.\nData Loader: An iterable (like Flux.DataLoader) providing training batches.\nLoss Function: A function f(ŷ, y) that returns a vector of losses for the batch.\nOptimizer: The optimizer state (from Flux.setup).\nQuantities: A list of metrics to monitor during training.","category":"section"},{"location":"architecture/#Training","page":"Architecture","title":"Training","text":"To start training with live monitoring, use the train! function:\n\ntrain!(learner, epochs, with_plots, track_every)\n\nlearner: Your Learner instance.\nepochs: Number of training epochs.\nwith_plots: Boolean. If true, starts a WGLMakie dashboard in your browser (or VS Code plot pane).\ntrack_every: Int. Number of steps for which quantities computation should be skipped (allows for speed-up) If 1, every step is tracked.","category":"section"},{"location":"architecture/#Available-Quantities","page":"Architecture","title":"Available Quantities","text":"The package provides several diagnostic quantities inspired by the \"Cockpit\" paper:\n\n| Quantity | Description | | :–- | :–- |ins | LossQuantity() | Tracks the training loss over time. | | GradNormQuantity() | Monitors the L2 norm of the gradients. | | DistanceQuantity() | Measures the L2 distance from the initialization. | | UpdateSizeQuantity() | Tracks the L2 norm of the parameter updates. | | NormTestQuantity() | Computes the \"norm test\" (checks if the gradient is dominated by noise). | | GradHist1dQuantity() | Visualizes the 1D distribution of gradient elements. |","category":"section"},{"location":"visualization/#Visualization","page":"Visualization","title":"Visualization","text":"This page describes the visualization components of the package.\n\n","category":"section"},{"location":"visualization/#Dashboard","page":"Visualization","title":"Dashboard","text":"The dashboard provides a live, interactive view of training dynamics and updates continuously while training is running.\n\nCurrently implemented plots:\n\nTraining loss vs Iteration\nParameter distance (L2 distance from initial weights)\nParameter update size (step-to-step distance)\nGradient norm\nGradient norm test (measuring signal-to-noise)\nGradient distribution (1D histogram)\n\nAll plots that use Iteration on the X-axis are automatically linked, ensuring a synchronized view of training progress across metrics. Axis limits are automatically adjusted during training to keep all data visible.\n\n","category":"section"},{"location":"visualization/#Makie-integration","page":"Visualization","title":"Makie integration","text":"The package uses Makie.jl and observable variables to enable live updates.","category":"section"},{"location":"visualization/#Dashboard-execution-(internal)","page":"Visualization","title":"Dashboard execution (internal)","text":"Visualization is handled by a dedicated render loop that updates plots continuously while training is running. This loop is decoupled from the optimization process to avoid blocking training.\n\nTraining and visualization communicate via a Channel. At each iteration, the training loop sends:\n\nthe current step index\na dictionary containing the values of the tracked quantities\n\nScalar quantities are appended to time series plots. Vector-valued quantities (such as gradient histograms) replace the current plot data.\n\nThe dashboard is rendered using WGLMakie and served in a browser via Bonito.","category":"section"},{"location":"#LMD4MLTraining","page":"Home","title":"LMD4MLTraining","text":"Documentation for LMD4MLTraining.\n\n","category":"section"},{"location":"#LMD4MLTraining.jl","page":"Home","title":"LMD4MLTraining.jl","text":"LMD4MLTraining.jl is a Julia package for live monitoring and visual debugging of neural network training in Flux.jl.\n\nThe package is inspired by the Python package cockpit and aims to provide insight into training dynamics by visualizing diagnostic quantities while training is running.\n\n","category":"section"},{"location":"#Motivation","page":"Home","title":"Motivation","text":"When training neural networks, issues such as unstable optimization, exploding gradients, or stalled learning are often only detected after training has finished. This package addresses this problem by providing live, interactive visualizations of important training metrics.\n\n","category":"section"},{"location":"#Features","page":"Home","title":"Features","text":"Currently implemented features include:\n\nIntegration with standard Flux/Zygote training loops\nLive visualization using WGLMakie.jl\nMonitoring of user defined quantities: loss, gradient norm, distance, update size, norm test and gradient history.\nModular design for adding additional quantities and visual instruments\n\n","category":"section"},{"location":"#Project-status","page":"Home","title":"Project status","text":"This package is under active development.\n\n","category":"section"},{"location":"#Documentation-overview","page":"Home","title":"Documentation overview","text":"Getting Started: how to install the package and run the provided example\nArchitecture: overview of the internal design and module structure\nQuantities: description of tracked training quantities\nVisualization: dashboard and plotting design\nAPI Reference: exported types and functions","category":"section"},{"location":"quantities/#Quantities","page":"Quantities","title":"Quantities","text":"Quantities are numerical diagnostics computed during training and logged for live visualization. They are evaluated once per optimization step (i.e. per batch).\n\nBelow is an overview of what is currently tracked and how to interpret it.","category":"section"},{"location":"quantities/#Implemented-quantities","page":"Quantities","title":"Implemented quantities","text":"","category":"section"},{"location":"quantities/#LossQuantity()","page":"Quantities","title":"LossQuantity()","text":"Tracks the mean batch loss.\n\nDefinition: mean(losses) where losses is the per-sample loss vector returned by your loss function.\nInterpretation:Measures how well the model fits the current batch and tells you whether training is making immediate progress. Should generally decrease over time (depending on learning rate schedule, regularization, etc.).","category":"section"},{"location":"quantities/#GradNormQuantity()","page":"Quantities","title":"GradNormQuantity()","text":"Tracks the L2 norm of the gradient of the batch loss.\n\nDefinition: ‖∇θ L‖₂ (computed as sqrt(sum(abs2, grads))) across all trainable parameters).\nInterpretation:\nSpikes can indicate exploding gradients or a too-large step size.\nVery small values can indicate vanishing gradients or stalled learning","category":"section"},{"location":"quantities/#DistanceQuantity()","page":"Quantities","title":"DistanceQuantity()","text":"Tracks the L2 distance from the initialization.\n\nDefinition: ‖θₜ - θ₀‖₂.\nInterpretation:\nHelps diagnose whether training is actually moving the parameters.\nUseful for comparing runs: some instabilities show up as rapid parameter drift.","category":"section"},{"location":"quantities/#UpdateSizeQuantity()","page":"Quantities","title":"UpdateSizeQuantity()","text":"Tracks the size of the applied parameter update in each step.\n\nDefinition: ‖θ_after - θ_before‖₂.\nInterpretation:\nUseful to spot overly aggressive updates and check if the optimizer is still actively moving or has entered a noisy diffusion regime","category":"section"},{"location":"quantities/#NormTestQuantity()","page":"Quantities","title":"NormTestQuantity()","text":"Tracks a normalized gradient noise estimate.\n\nDefinition (batch size B):\nCompute per-sample gradients gᵢ via a pullback on the per-sample loss vector.\nLet g be the summed batch gradient used for the optimizer step.\nReport\n1 / (B (B-1)) * ( (∑ᵢ ‖gᵢ‖₂²) / ‖g‖₂² - B ).\nInterpretation:\nLarger values suggest the batch gradient is dominated by noise/variance across samples. Helps diagnose batch size issues and unstable optimization.\nNear-zero values suggest per-sample gradients are relatively aligned.","category":"section"},{"location":"quantities/#GradHist1dQuantity(;-nbins100,-maxval0.05)","page":"Quantities","title":"GradHist1dQuantity(; nbins=100, maxval=0.05)","text":"Tracks a 1D histogram of gradient element values.\n\nParameters:\nnbins: number of histogram bins.\nmaxval: clamp range for gradient elements (values outside [-maxval, maxval] are clamped).\nInterpretation: helps detect issues like poor data scaling, vanishing/exploding gradients, or dead layers\nCost note: This quantity uses per-sample pullbacks and iterates over all gradient arrays, so it can be noticeably more expensive than scalar quantities.","category":"section"},{"location":"quantities/#Creating-new-quantities","page":"Quantities","title":"Creating new quantities","text":"To create a new quantity, subtype AbstractQuantity and implement the following functions:\n\nquantity_key(q::MyQuantity): returns a unique symbol key.\ncompute(q::MyQuantity, losses, back, grads, params): returns the computed value.\nAdd it to the dashboard layout in src/instruments/dashboard.jl.","category":"section"}]
}
